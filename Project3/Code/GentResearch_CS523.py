# -*- coding: utf-8 -*-
"""MyGent_467.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m5quQtpeq57lph34kVP7a8xX9wy741uR

# Project 3
# Gentrification within the US
CS 423/523

Damian Franco & Meiling Traeger

# Core causes for Gentrification
 - Higher income residents
 - Rent/Wage gap
 - Real estate investments
 - Higher income residents moving in
 - Pricier business locations
 - Education level
 - Racial demographic
 - Historic conditions (policies and practices that could make communities susceptible)
 - Rising median household cost

# Charactristics of Gentrification
 - Increased investment in neighborhood amenities, like transit and parks
 - Industrial land could change to restaurants and storefronts
 - Investors flipping properties for large profits
 - High-end development which leads to landlords looking for higher-paying tenants
 - Higher median household cost

# Measurements of Gentrification
 - Census data to measure changes in neighborhood composition by income
 - Race
 - Education
 - Housing value

## Current datasets

### All datasets provided by the US Census

- Population/Racial demographic
 - [https://data.census.gov/cedsci/table?q=race&g=0500000US35001&tid=ACSDT1Y2019.B02001](https://data.census.gov/cedsci/table?q=race&g=0500000US35001&tid=ACSDT1Y2019.B02001)
- Median house price
 - [https://data.census.gov/cedsci/table?q=B25077%3A MEDIAN VALUE (DOLLARS)&g=0500000US35001&tid=ACSDT1Y2019.B25077](https://data.census.gov/cedsci/table?q=B25077%3A%20MEDIAN%20VALUE%20%28DOLLARS%29&g=0500000US35001&tid=ACSDT1Y2019.B25077)
- Education Data
 - https://data.census.gov/cedsci/table?q=Educational%20Attainment&g=0500000US35001&tid=ACSDT1Y2019.C15003
- Income/Employment data
 - [https://data.census.gov/cedsci/table?q=Income (Households, Families, Individuals)&g=0500000US35001&tid=ACSST1Y2019.S1901&hidePreview=true](https://data.census.gov/cedsci/table?q=Income%20%28Households,%20Families,%20Individuals%29&g=0500000US35001&tid=ACSST1Y2019.S1901&hidePreview=true)
    
# Ideas
 - Data from County or Zip code or full city? I personally say county but has to line up with other data
  - LA County data: https://data.census.gov/cedsci/profile?g=0600000US0603791750
 - Potential Causes:
  - Size of household
  - Proximity to city transit
  - What is an "atractive" business
  - Non-english speakers
  - Cause? Price of inner-city housing to decline to the point where it becomes desirable for outsiders to buy it and convert it to a higher value use
  - As wealthier homeowners move in, old houses get fixed up, the aesthetics of the neighborhood improve, and more businesses spring up to serve the new residents.  All of this makes the neighborhood even more attractive to potential (wealthy) buyers. Once a neighborhood begins to gentrify, it can take on a new character that attracts like-minded people
  - Neighborhoods impacted by gentrification have been shaped historically by decades of discriminatory public policies and private real estate practices that undermined property values, facilitated substandard living conditions, and generated racially segregated housing patterns. These neighborhoodsâ€™ lower property values, location in the urban core near good jobs and transit, and historical and cultural character are all factors that are making them more attractive to newcomers and susceptible to redevelopment

## Links
 - https://www.urbandisplacement.org/about/what-are-gentrification-and-displacement/ 
 - https://www.vox.com/22629826/gentrification-definition-housing-racism-segregation-cities
 - https://dspace.mit.edu/handle/1721.1/123884 
 - https://www.elca.org/JLE/Articles/1135
 - https://sites.utexas.edu/gentrificationproject/understanding-gentrification-and-displacement/

#TODO:
 - Shannon Entropy calculation (DONE)
 - Transfer Entropy calculation
 - Mutual information calculation
 - Plot all the findings
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
from mpl_toolkits.mplot3d import Axes3D      
import numpy as np                    
import os 
import seaborn as sbs

years = [*range(2010,2020)]
race = {}
median_house = {}
education = {}
income = {}

"""# Create and clean racial demographic data"""

race_clean_col = {'id': 'ID',
             'Geographic Area Name': 'County',
             'Estimate!!Total': 'Total Population',
             'Estimate!!Total:': 'Total Population',
             'Estimate!!Total!!White alone': 'White alone',
             'Estimate!!Total:!!White alone': 'White alone',
             'Estimate!!Total!!Black or African American alone': 'Black or African American alone',
             'Estimate!!Total:!!Black or African American alone': 'Black or African American alone',
             'Estimate!!Total!!American Indian and Alaska Native alone': 'American Indian and Alaska Native along',
             'Estimate!!Total:!!American Indian and Alaska Native alone': 'American Indian and Alaska Native along',
             'Estimate!!Total!!Asian alone': 'Asain alone',
             'Estimate!!Total:!!Asian alone': 'Asain alone',
             'Estimate!!Total!!Native Hawaiian and Other Pacific Islander alone': 'Native Hawaiian and Other Pacific Islander alone',
             'Estimate!!Total:!!Native Hawaiian and Other Pacific Islander alone': 'Native Hawaiian and Other Pacific Islander alone',
             'Estimate!!Total!!Some other race alone': 'Some other race alone',
             'Estimate!!Total:!!Some other race alone': 'Some other race alone',
             'Estimate!!Total!!Two or more races': 'Two or more races',
             'Estimate!!Total:!!Two or more races:': 'Two or more races',
             'Estimate!!Total!!Two or more races!!Two races including Some other race': 'Two races including some other race',
             'Estimate!!Total:!!Two or more races:!!Two races including Some other race': 'Two races including some other race',
             'Estimate!!Total!!Two or more races!!Two races excluding Some other race, and three or more races': 'Two races excluding Some other race, and three or more races',
             'Estimate!!Total:!!Two or more races:!!Two races excluding Some other race, and three or more races': 'Two races excluding Some other race, and three or more races'
             }

# Read in ACS race Demographics by Year
for year in years: # Each year from 2010-2019
  new_df = pd.read_csv(f'/content/drive/MyDrive/GentrificationProj/Race/ACS_county{year}.csv', delimiter=',', header=1) # read in the CSV file
  new_df.dataframeName = f"ACS County Demographics {year}"  # Name of the dataframe
  cols = range(3, new_df.shape[1], 2)                       # Indecies of margin of error columns
  new_df.drop(new_df.columns[cols],axis=1,inplace=True)     # Remove all margin of error columns
  new_df.rename(columns=race_clean_col,inplace=True)            # Clean the column names so they are more readable
  new_df.dropna(inplace=True)                               # Remove NA Elements
  new_df = new_df[~new_df["County"].str.contains('Puerto Rico')] # Remove Puerto Rico, only want to look at mainland US.
  race[year] = new_df                                        # Assign to dictionary

# Save racial demographic data as a .txt file
textRace = {}
textRace = race[2019].copy()

textCols = range(0, 2, 1)                       # Indecies of margin of error columns
textRace.drop(textRace.columns[textCols],axis=1,inplace=True)     # Remove all margin of error columns

np.savetxt('RaceDemo2019.txt', textRace.values, fmt='%s', delimiter='\t')
#files.download("RaceDemo2019.txt")

"""# Create and clean median house price data"""

median_clean_col = {'id': 'ID',
                    'Geographic Area Name': 'County',
                    'Estimate!!Median value (dollars)': 'Median House Value (Dollars)'
                    }

# Read in ACS Demographics by Year
for year in years: # Each year from 2010-2019
  new_df = pd.read_csv(f'/content/drive/MyDrive/GentrificationProj/MedianValues/Median_{year}.csv', delimiter=',', header=1) # read in the CSV file
  new_df.dataframeName = f"Median House Price {year}"  # Name of the dataframe
  cols = range(3, new_df.shape[1], 2)                       # Indecies of margin of error columns
  new_df.drop(new_df.columns[cols],axis=1,inplace=True)     # Remove all margin of error columns
  new_df.rename(columns=median_clean_col,inplace=True)            # Clean the column names so they are more readable
  new_df.dropna(inplace=True)                               # Remove NA Elements
  new_df = new_df[~new_df["County"].str.contains('Puerto Rico')] # Remove Puerto Rico, only want to look at mainland US
  median_house[year] = new_df                                        # Assign to dictionary

"""# Create and clean education data"""

education_clean_col = {'id': 'ID',
                       'Geographic Area Name': 'County',
                       'NAME': 'County',
                       'Estimate!!Total': 'Total',
                       'Total!!Estimate!!Population 18 to 24 years': 'Population 18 to 24 years',
                       'Estimate!!Total!!Population 18 to 24 years': 'Population 18 to 24 years',
                       'Estimate!!Total!!AGE BY EDUCATIONAL ATTAINMENT!!Population 18 to 24 years': 'Population 18 to 24 years',
                       'Total!!Estimate!!Population 18 to 24 years!!Less than high school graduate': 'Less than high school graduate (Ages 18-24)',
                       'Total!!Estimate!!Population 18 to 24 years!!High school graduate (includes equivalency)': 'High school graduate (includes equivalency) (Ages 18-24)',
                       'Total!!Estimate!!Population 18 to 24 years!!Some college or associate\'s degree': 'Some college or associate\'s degree (Ages 18-24)',
                       'Total!!Estimate!!Population 18 to 24 years!!Bachelor\'s degree or higher': 'Bachelor\'s degree or higher (Ages 18-24)',
                       'Total!!Estimate!!Bachelor\'s degree or higher': 'Bachelor\'s degree or higher (Ages 18-24)',
                       'Estimate!!Total!!Population 18 to 24 years!!Bachelor\'s degree or higher': 'Bachelor\'s degree or higher (Ages 18-24)',
                       'Estimate!!Total!!AGE BY EDUCATIONAL ATTAINMENT!!Population 18 to 24 years!!Bachelor\'s degree or higher': 'Bachelor\'s degree or higher (Ages 18-24)',
                       'Total!!Estimate!!Population 25 years and over': 'Population 25 years and over',
                       'Estimate!!Total!!Population 25 years and over': 'Population 25 years and over',
                       'Estimate!!Total!!AGE BY EDUCATIONAL ATTAINMENT!!Population 25 years and over': 'Population 25 years and over',
                       'Total!!Estimate!!Population 25 years and over!!Less than 9th grade': 'Less than 9th grade (Ages 25 and over)',
                       'Total!!Estimate!!Population 25 years and over!!9th to 12th grade, no diploma': '9th to 12th grade(Ages 25 and over)',
                       'Total!!Estimate!!Population 25 years and over!!High school graduate (includes equivalency)': 'High school graduate (includes equivalency) (Ages 25 and over)',
                       'Total!!Estimate!!Population 25 years and over!!Some college, no degree': 'Some college, no degree (Ages 25 and over)',
                       'Total!!Estimate!!Population 25 years and over!!Associate\'s degree': 'Associate\'s degree (Ages 25 and over)',
                       'Total!!Estimate!!Population 25 years and over!!Bachelor\'s degree': 'Bachelor\'s degree (Ages 25 and over)',
                       'Total!!Estimate!!Bachelor\'s degree': 'Bachelor\'s degree (Ages 25 and over)',
                       'Estimate!!Total!!Population 25 years and over!!Bachelor\'s degree': 'Bachelor\'s degree (Ages 25 and over)',
                       'Estimate!!Total!!AGE BY EDUCATIONAL ATTAINMENT!!Population 25 years and over!!Bachelor\'s degree': 'Bachelor\'s degree (Ages 25 and over)',
                       'Total!!Estimate!!Graduate or professional degree': 'Graduate or professional degree (Ages 25 and over)',
                       'Estimate!!Total!!Population 25 years and over!!Graduate or professional degree': 'Graduate or professional degree (Ages 25 and over)',
                       'Estimate!!Total!!AGE BY EDUCATIONAL ATTAINMENT!!Population 25 years and over!!Graduate or professional degree': 'Graduate or professional degree (Ages 25 and over)',
                       'Total!!Estimate!!Population 25 years and over!!Graduate or professional degree': 'Graduate or professional degree (Ages 25 and over)',
                      }

# Read in ACS Demographics by Year
for year in years: # Each year from 2010-2019
  df_race = race[year]
  new_df = pd.read_csv(f'/content/drive/MyDrive/GentrificationProj/Education/Education_{year}.csv', delimiter=',', header=1) # read in the CSV file
  new_df.dataframeName = f"Education of {year}"  # Name of the dataframe
  cols = range(3, new_df.shape[1], 2)                       # Indecies of margin of error columns
  new_df.drop(new_df.columns[cols],axis=1,inplace=True)     # Remove all margin of error columns
  new_df.rename(columns=education_clean_col,inplace=True)   # Clean the column names so they are more readable
  new_df.dropna(inplace=True)                               # Remove NA Elements
  new_df['Bachelor\'s degree or higher (Ages 25 and over)'] = new_df['Bachelor\'s degree (Ages 25 and over)'] + new_df['Graduate or professional degree (Ages 25 and over)']
  if(year >= 2015):
      drop_cols = range(15,386,1)
      percent18 = new_df['Bachelor\'s degree or higher (Ages 18-24)'] / df_race['Total Population']
      percent18 = percent18 * 100
      percent25 = new_df['Bachelor\'s degree or higher (Ages 25 and over)'] / df_race['Total Population']
      percent25 = percent25 * 100
      new_df['Bachelor\'s degree or higher (%)'] =  percent18 + percent25
      new_df.drop(new_df.columns[drop_cols],axis=1,inplace=True) # Remove all unnecessary columns
      drop_cols2 = range(3,6,1)
      drop_cols3 = range(5,10,1)
      new_df.drop(new_df.columns[drop_cols2],axis=1,inplace=True) # Remove all unnecessary columns
      new_df.drop(new_df.columns[drop_cols3],axis=1,inplace=True) # Remove all unnecessary columns
      drop_cols4 = range(2,8,1)
      new_df.drop(new_df.columns[drop_cols4],axis=1,inplace=True) # Remove all unnecessary columns
  else:
      full_percent = new_df['Bachelor\'s degree or higher (Ages 18-24)'] + new_df['Bachelor\'s degree or higher (Ages 25 and over)']
      full_percent = full_percent / 2
      new_df['Bachelor\'s degree or higher (%)'] = full_percent
      drop_cols = range(2,117,1)
      new_df.drop(new_df.columns[drop_cols],axis=1,inplace=True) # Remove all unnecessary columns
  new_df = new_df[~new_df["County"].str.contains('Puerto Rico')] # Remove Puerto Rico, only want to look at mainland US
  education[year] = new_df                                        # Assign to dictionary

"""# Create and clean income data"""

income_clean_col = {'id': 'ID',
                    'Geographic Area Name': 'County',
                    'Households!!Estimate!!Total': 'Total households',
                    'Estimate!!Households!!Total': 'Total households',
                    'Households!!Estimate!!Median income (dollars)': 'Median Income (Dollars)',
                    'Estimate!!Households!!Median income (dollars)': 'Median Income (Dollars)'
                    }

# Read in ACS Demographics by Year
for year in years: # Each year from 2010-2019
  new_df = pd.read_csv(f'/content/drive/MyDrive/GentrificationProj/Income/Income_{year}.csv', delimiter=',', header=1) # read in the CSV file
  new_df.dataframeName = f"Income for {year}"  # Name of the dataframe
  cols = range(3, new_df.shape[1], 2)                       # Indecies of margin of error columns
  new_df.drop(new_df.columns[cols],axis=1,inplace=True)     # Remove all margin of error columns
  drop_cols1 = range(3,13,1)
  new_df.drop(new_df.columns[drop_cols1],axis=1,inplace=True) # Remove all unnecessary columns
  drop_cols2 = range(4,56,1)
  new_df.drop(new_df.columns[drop_cols2],axis=1,inplace=True) # Remove all unnecessary columns
  new_df.rename(columns=income_clean_col,inplace=True)      # Clean the column names so they are more readable
  new_df.dropna(inplace=True)                               # Remove NA Elements
  new_df = new_df[~new_df["County"].str.contains('Puerto Rico')] # Remove Puerto Rico, only want to look at mainland US
  income[year] = new_df                                          # Assign to dictionary

"""# Checking data"""

race[2019].head()

median_house[2016].head()

education[2010].head()

income[2016].head()

"""# Merge datasets"""

merged_df = {}

for year in years:
  df = pd.concat([median_house[year].set_index('ID'),
                  income[year].set_index('ID'),
                  education[year].set_index('ID'),
                  race[year].set_index('ID'),
                  ],
                               
                              axis=1,
                              join='inner')
  
  df = df.loc[:,~df.columns.duplicated()] # Remove duplicate columns
  merged_df[year] = df

merged_df[2015].head()

"""# Find Eligible Counties

## Eligablity
## Methods
Based on [Gentrification and
Disinvestment 2020](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/ac5291fd-dcc5-4927-a73a-e2ce3858efa1/Gentrification-and-Opportunity-Zones-2020-v9.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20211205%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20211205T201043Z&X-Amz-Expires=86400&X-Amz-Signature=6b110a3bd2669cc2ec39b8835a4a1376f7d3b6adb33d40db0d3c97ef6abce4bd&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Gentrification-and-Opportunity-Zones-2020-v9.pdf%22&x-id=GetObject)

### Criteria of Gentrification
Eligible
* **Population** > 500
* **Median Home Value** < 40th percentile
* **Median Household Income** < 40th Percentile

Gentrified
* **Increase in Median Home Value** > 60th Percentile
* **Increase in College Educated** > 60th percentile
* **Increase in median household income**
"""

# Helper Functions

R_SUFFIX = 'old'

def income_increased(row):
  med_income_new = row['Median Income (Dollars)']
  med_income_prev = row[f'Median Income (Dollars){R_SUFFIX}']

  if(med_income_new > med_income_prev):
    return 1
  else:
    return 0

def is_eligble(row, home_val_40perc, med_income_40perc):
  med_home_val = row['Median House Value (Dollars)']
  med_income = row['Median Income (Dollars)']
  pop = row['Total Population']

  if(med_home_val < home_val_40perc and 
     med_income < med_income_40perc and
     pop > 500):
    return 1
  else:
    return 0

def is_gentrified(row, home_val_60perc, college_edu_60perc):
  med_home_val = row['Median House Value (Dollars)']
  college_edu = row['Bachelor\'s degree or higher (%)']
  increased_income = row['Increased Income'] 

  if(med_home_val > home_val_60perc and 
     college_edu > college_edu_60perc and
      increased_income):
    return 1
  else: 
    return 0

# Determined whether an a county's median income increased from previous year

for year in years[1:]:
  df = merged_df[year].copy()
  df_prev = merged_df[year-1].copy()
  df = df[['Median Income (Dollars)']]
  df_prev = df_prev[['Median Income (Dollars)']]
  temp_df = df.join(df_prev, on='ID', rsuffix=R_SUFFIX)
  temp_df['Increased Income'] = temp_df.apply(lambda row: income_increased(row), axis=1)

  temp_df = temp_df[['Increased Income']]

  merged_df[year] = merged_df[year].merge(temp_df, on='ID')

years = years[1:]

for year in years:
  df = merged_df[year]
  home_val_60perc = df['Median House Value (Dollars)'].quantile(0.6)
  home_val_40perc = df['Median House Value (Dollars)'].quantile(0.4)
  med_income_40perc = df['Median Income (Dollars)'].quantile(0.4) 
  college_edu_60perc = df['Bachelor\'s degree or higher (%)'].quantile(0.6)

  df['home_val_percentile'] = home_val_40perc
  df['Eligible'] = df.apply(lambda row: is_eligble(row, home_val_40perc, med_income_40perc), axis=1)
  df['Gentrified'] = df.apply(lambda row: is_gentrified(row, home_val_60perc, college_edu_60perc), axis=1)

# Check eligible column
merged_df[2010].head()

"""## Confirmed with the case:
2010
 * Median home value (40th Percentile) - 145400
 * Median income (40th Percentile) - 44996

Baldwin County, Alabama - 2010
 * Population - 200111
 * Median home value - 176100
 * Median income - 47502

Checking statistics
 * **200111** > 500
 * **176100** > 145400
 * **47502** > 44996

## Outcome - NOT ELIGIBLE
"""

# Observe how many counties are eligible
# 0 = Not eligible
# 1 = Eligible
merged_df[2018].Eligible.value_counts()



"""# Check Albuquerque for gentrification"""

for year in years:
  abq = merged_df[year].loc['0500000US35001']
  print(f"Eligible ({year}): {abq.Eligible}")

for year in years:
  abq = merged_df[year].loc['0500000US32003']
  print(f"Gentrified ({year}): {abq.Gentrified}")

"""# Create eligible only dataset"""

# Create dataset with only eligible counties
eligible_df = {}
for year in years:
  df = merged_df[year]
  df = df[df.Gentrified == 0]
  eligible_df[year] = df

# Check eligible dataset
eligible_df[2014].head()

from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score

#curr_year = merged_df[2017]
#predict_val = curr_year['Median House Value (Dollars)']

#predict_val_train = predict_val[:-20]
#predict_val_test = predict_val[-20:]

#target_train = predict_val[:-20]
#target_test = predict_val[-20:]

#regr = linear_model.LinearRegression()

#regr.fit(predict_val_train, target_train)
#output = regr.predict(predict_val_test)

#plt.scatter(predict_val_test, target_test, color="black")
#plt.plot(predict_val_test, output, color="blue", linewidth=3)

"""# Visualizing Data"""

import matplotlib.pyplot as plt 
from google.colab import files

"""## Visualizing stats"""

cols = ['Year', 'Total Counties', 'Gentrified', 'Eligble', 'Neither']

stats = []

for year in years:
  df = merged_df[year]
  total_counties = df.shape[0]
  gentrified_count = (df['Gentrified'].values == 1).sum()
  eligble_count = (df['Eligible'].values == 1).sum()
  neither = len(df[(df['Eligible'] == 0) & (df['Gentrified'] == 0)])
  ls = [year, total_counties, gentrified_count, eligble_count, neither]
  stats.append(ls)

stats = pd.DataFrame(stats, columns=cols)
stats

plt.plot(stats['Year'], stats['Gentrified'], label = "Gentrified")
plt.plot(stats['Year'], stats['Eligble'], label = "Eligible")
plt.plot(stats['Year'], stats['Neither'], label = "Neither")
plt.legend()
plt.ylim(0, 450)
plt.xlabel("Year", fontweight ='bold', fontsize = 12)
plt.ylabel("Number of Counties", fontweight ='bold', fontsize = 12)
plt.title("Gentrified, eligible or neither trends")
plt.show()

"""## Visualizing percentiles"""

# Plotting the 40th percentile house prices per year
plt.clf()
home_val_40perc = list()
med_income_40perc = list()
med_income_60perc = list()
education_60perc = list()
for year in years:
  home_val_40perc.append(merged_df[year]['Median House Value (Dollars)'].quantile(0.4))
  med_income_40perc.append(merged_df[year]['Median Income (Dollars)'].quantile(0.4))
  med_income_60perc.append(merged_df[year]['Median Income (Dollars)'].quantile(0.6))
  education_60perc.append(merged_df[year]['Bachelor\'s degree or higher (%)'].quantile(0.6))

plt.plot(years, home_val_40perc, label = "Median Home Value")
plt.plot(years, med_income_40perc, label= "Median Income")
plt.legend()
plt.xlabel("Year", fontweight ='bold', fontsize = 12)
plt.ylabel("USD", fontweight ='bold', fontsize = 12)
plt.title("Median Home Value and Income (40th Percentile)", fontsize = 10)
plt.ticklabel_format(style='plain')
plt.ylim(0,200000)
plt.show()

from scipy.stats import gaussian_kde
y_edu = []

for year in years:
  df = merged_df[year]
  y_edu.append(df['Bachelor\'s degree or higher (%)'].tolist())

X_edu = [years[i] for i, data in enumerate(y_edu) for j in range(len(data))]
Y_edu = [val for data in y_edu for val in data]
density = np.vstack([X_edu, Y_edu])
z = gaussian_kde(density)(density)

plt.figure(figsize=(9, 6))
plt.scatter(X_edu,Y_edu,c=z)
plt.plot(years, education_60perc)
plt.ylim(0, 80)
plt.xlabel("Year", fontweight ='bold', fontsize = 12)
plt.ylabel("Bachelor\'s degree or higher (% of population)", fontweight ='bold', fontsize = 10)
plt.title("Education (60th percentile)")
plt.show()

y = []

for year in years:
  df = merged_df[year]
  y.append(df['Median Income (Dollars)'].tolist())

X = [years[i] for i, data in enumerate(y) for j in range(len(data))]
Y = [val for data in y for val in data]
density = np.vstack([X, Y])
z = gaussian_kde(density)(density)

plt.figure(figsize=(9, 6))
plt.scatter(X,Y,c=z)
plt.xlabel('Year', fontweight = 'bold', fontsize = 12)
plt.ylabel('USD', fontweight = 'bold', fontsize = 12)
plt.xticks(years)
plt.plot(years, med_income_40perc, color='red', label='40th Percentile')
plt.plot(years, med_income_60perc, color='green', label='60th Percentile')
plt.title('Median Income density')
plt.ticklabel_format(style='plain')
plt.ylim(0,180000)
plt.legend()
plt.show()

"""## Visualizing an eligible county data"""

# County is currently:
county_code = '0500000US35005'

pop = []
# Plotting a given county code house prices
for year in years:
  county = eligible_df[year].loc[county_code]
  pop.append(county['Total Population'])

plt.xlabel('Year', fontweight ='bold', fontsize = 15)
plt.ylabel('Total Population', fontweight ='bold', fontsize = 15)
plt.plot(years, pop, '-')

home_vals = []
# Plotting a given county code house prices
for year in years:
  county = eligible_df[year].loc[county_code]
  home_vals.append(county['Median House Value (Dollars)'])

plt.xlabel('Year', fontweight ='bold', fontsize = 15)
plt.ylabel('Median Home Value', fontweight ='bold', fontsize = 15)
plt.plot(years, home_vals, '-')

avg_income = []
# Plotting a given county code median income
for year in years:
  county = eligible_df[year].loc[county_code]
  avg_income.append(county['Median Income (Dollars)'])

plt.xlabel('Year', fontweight ='bold', fontsize = 15)
plt.ylabel('Median Income', fontweight ='bold', fontsize = 15)
plt.plot(years, avg_income, '-')

education = []
# Plotting a given county code median income
for year in years:
  county = eligible_df[year].loc[county_code]
  education.append(county['Bachelor\'s degree or higher (%)'])

plt.xlabel('Year', fontweight ='bold', fontsize = 15)
plt.ylabel('Bachelor\'s degree or higher (%)', fontweight ='bold', fontsize = 15)
plt.ylim(0,100)
plt.plot(years, education, '-')

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
X = np.arange(4)

white_alone = []
black_or_aa = []
usa_ind_alaska = []
asian = []
hawaii_island = []
some_other_alone = []
two_more = []
two_including = []
two_excluding = []
# Plotting the racial demographic given the year
for year in years:
  county = eligible_df[year].loc[county_code]
  white_alone.append(county['White alone'])
  black_or_aa.append(county['Black or African American alone'])
  usa_ind_alaska.append(county['American Indian and Alaska Native along'])
  asian.append(county['Asain alone'])
  hawaii_island.append(county['Native Hawaiian and Other Pacific Islander alone'])
  some_other_alone.append(county['Some other race alone'])
  two_more.append(county['Two or more races']) 
  two_including.append(county['Two races including some other race'])
  two_excluding.append(county['Two races excluding Some other race, and three or more races'])

plt.xlabel('Year', fontweight ='bold', fontsize = 15)
plt.ylabel('White alone', fontweight ='bold', fontsize = 15)
ax.bar(years,white_alone)
#ax.bar(X + 0.00, white_alone[0], color = 'b', width = 0.25)
#ax.bar(X + 0.25, black_or_aa[], color = 'g', width = 0.25)
#ax.bar(X + 0.50, usa_ind_alaska[0], color = 'r', width = 0.25)

#county.plot(x=years, y=[white_alone, black_or_aa, usa_ind_alaska, asian, hawaii_island, some_other_alone, two_more, two_including, two_excluding], kind="bar")
#plt.show()

"""## Visualizing a non-eligible county data"""

# County is currently:
county_code = '0500000US35001'

pop = []
# Plotting a given county code house prices
for year in years:
  county = merged_df[year].loc[county_code]
  pop.append(county['Total Population'])

plt.xlabel('Year', fontweight ='bold', fontsize = 15)
plt.ylabel('Total Population', fontweight ='bold', fontsize = 15)
plt.plot(years, pop, '-')

home_vals = []
# Plotting a given county code house prices
for year in years:
  county = merged_df[year].loc[county_code]
  home_vals.append(county['Median House Value (Dollars)'])

plt.xlabel('Year', fontweight ='bold', fontsize = 15)
plt.ylabel('Median Home Value', fontweight ='bold', fontsize = 15)
plt.plot(years, home_vals, '-')

avg_income = []
# Plotting a given county code median income
for year in years:
  county = merged_df[year].loc[county_code]
  avg_income.append(county['Median Income (Dollars)'])

plt.xlabel('Year', fontweight ='bold', fontsize = 15)
plt.ylabel('Median Income', fontweight ='bold', fontsize = 15)
plt.plot(years, avg_income, '-')

education = []
# Plotting a given county code median income
for year in years:
  county = merged_df[year].loc[county_code]
  education.append(county['Bachelor\'s degree or higher (%)'])

plt.xlabel('Year', fontweight ='bold', fontsize = 15)
plt.ylabel('Bachelor\'s degree or higher (%)', fontweight ='bold', fontsize = 15)
plt.ylim(0,100)
plt.plot(years, education, '-')

# Plotting the racial demographic given the year
#for year in years:
county = merged_df[2015].loc[county_code]
white_alone = (county['White alone'])
black_or_aa = (county['Black or African American alone'])
usa_ind_alaska = (county['American Indian and Alaska Native along'])
asian = (county['Asain alone'])
hawaii_island = (county['Native Hawaiian and Other Pacific Islander alone'])
some_other_alone = (county['Some other race alone'])
two_more = (county['Two or more races']) 
two_including = (county['Two races including some other race'])
two_excluding = (county['Two races excluding Some other race, and three or more races'])

bars = ['White alone', 'Black or African American alone', 
        'American Indian and Alaska Native alone', 'Asain alone', 
        'Native Hawaiian and Other Pacific Islander alone', 'Some other race alone', 'Two or more races',
        'Two races including some other race', 'Two races excluding Some other race']
race_arr = [white_alone, black_or_aa, usa_ind_alaska, asian, 
            hawaii_island, some_other_alone, two_more, two_including, two_excluding]
X = np.arange(9)
plt.figure(figsize=(35, 3))
plt.bar(X, race_arr, width = 0.5, 
       color=['yellow', 'red', 'green', 'blue', 'cyan', 'gold', 'magenta', 'orange', 'indigo'], edgecolor='blue',
       align='center')

plt.xticks(X, bars)

"""# Visualizing every County




"""

# Counties is currently:
num_counties = 200

cols = ['Year', 'Median House Value (Dollars)']
all_cols = ['ID']
county_code = '0500000US06037'
curr_county = []

#all_counties = merged_df[2019].iloc[0]
#print(all_counties)

#county_codes = all_counties['ID']
#print(county_codes)

for year in years:
  county = merged_df[year].loc[county_code]
  curr_homeval = county['Median House Value (Dollars)']
  #avg_income1.append(county1['Median Income (Dollars)'])
  ls = [year, curr_homeval]
  curr_county.append(ls)

curr_county = pd.DataFrame(curr_county, columns=cols)

plt.plot(years, home_vals1, label = name1)
plt.plot(years, home_vals2, label = name2)
plt.plot(years, home_vals3, label = name3)
plt.legend()
plt.xlabel('Year', fontweight ='bold', fontsize = 12)
plt.ylabel('Median Home Value', fontweight ='bold', fontsize = 12)
plt.title("County vs County Median Home Value trends")
plt.ticklabel_format(style='plain')
plt.ylim(0,1500000)
plt.show()

plt.plot(years, avg_income1, label = name1)
plt.plot(years, avg_income2, label = name2)
plt.plot(years, avg_income3, label = name3)
plt.legend()
plt.xlabel('Year', fontweight ='bold', fontsize = 12)
plt.ylabel('Median Income', fontweight ='bold', fontsize = 12)
plt.title("County vs County Median Household Income trends")
plt.ticklabel_format(style='plain')
plt.ylim(0,200000)
plt.show()

plt.plot(years, education1, label = name1)
plt.plot(years, education2, label = name2)
plt.plot(years, education3, label = name3)
plt.legend()
plt.xlabel('Year', fontweight ='bold', fontsize = 12)
plt.ylabel('Bachelor\'s degree or higher (%)', fontweight ='bold', fontsize = 12)
plt.title("County vs County Education trends")
plt.ticklabel_format(style='plain')
plt.ylim(0,60)
plt.show()

"""## Visualizing County vs County"""

# Counties is currently:
county_code_1 = '0500000US06037'
county_code_2 = '0500000US55079'
county_code_3 = '0500000US08031'
county_code_4 = '0500000US06075'
county_code_5 = '0500000US35001'

name1 = ""
name2 = ""
name3 = ""
name4 = ""
name5 = ""
home_vals1 = []
home_vals2 = []
home_vals3 = []
home_vals4 = []
home_vals5 = []
avg_income1 = []
avg_income2 = []
avg_income3 = []
avg_income4 = []
avg_income5 = []
education1 = []
education2 = []
education3 = []
education4 = []
education5 = []

# Plotting a given county code house prices
for year in years:
  county1 = merged_df[year].loc[county_code_1]
  county2 = merged_df[year].loc[county_code_2]
  county3 = merged_df[year].loc[county_code_3]
  county4 = merged_df[year].loc[county_code_4]
  county5 = merged_df[year].loc[county_code_5]
  name1 = county1.County
  name2 = county2.County
  name3 = county3.County
  name4 = county4.County
  name5 = county5.County
  home_vals1.append(county1['Median House Value (Dollars)'])
  home_vals2.append(county2['Median House Value (Dollars)'])
  home_vals3.append(county3['Median House Value (Dollars)'])
  home_vals4.append(county4['Median House Value (Dollars)'])
  home_vals5.append(county5['Median House Value (Dollars)'])
  avg_income1.append(county1['Median Income (Dollars)'])
  avg_income2.append(county2['Median Income (Dollars)'])
  avg_income3.append(county3['Median Income (Dollars)'])
  avg_income4.append(county4['Median Income (Dollars)'])
  avg_income5.append(county5['Median Income (Dollars)'])
  education1.append(county1['Bachelor\'s degree or higher (%)'])
  education2.append(county2['Bachelor\'s degree or higher (%)'])
  education3.append(county3['Bachelor\'s degree or higher (%)'])
  education4.append(county4['Bachelor\'s degree or higher (%)'])
  education5.append(county5['Bachelor\'s degree or higher (%)'])

plt.plot(years, home_vals1, label = name1)
plt.plot(years, home_vals2, label = name2)
plt.plot(years, home_vals3, label = name3)
plt.plot(years, home_vals4, label = name4)
plt.plot(years, home_vals5, label = name5)
plt.legend()
plt.xlabel('Year', fontweight ='bold', fontsize = 12)
plt.ylabel('Median Home Value', fontweight ='bold', fontsize = 12)
plt.title("Sample Median Home Values")
plt.ticklabel_format(style='plain')
plt.ylim(0,1900000)
plt.show()

plt.plot(years, avg_income1, label = name1)
plt.plot(years, avg_income2, label = name2)
plt.plot(years, avg_income3, label = name3)
plt.plot(years, avg_income4, label = name4)
plt.plot(years, avg_income5, label = name5)
plt.legend()
plt.xlabel('Year', fontweight ='bold', fontsize = 12)
plt.ylabel('Median Income', fontweight ='bold', fontsize = 12)
plt.title("Sample Median Household Income")
plt.ticklabel_format(style='plain')
plt.ylim(0,200000)
plt.show()

plt.plot(years, education1, label = name1)
plt.plot(years, education2, label = name2)
plt.plot(years, education3, label = name3)
plt.plot(years, education4, label = name4)
plt.plot(years, education5, label = name5)
plt.legend()
plt.xlabel('Year', fontweight ='bold', fontsize = 12)
plt.ylabel('Bachelor\'s degree or higher (%)', fontweight ='bold', fontsize = 12)
plt.title("Sample Education")
plt.ticklabel_format(style='plain')
plt.ylim(0,80)
plt.show()

"""# Save fig copy and paste"""

# Save Plot
#plt.savefig("PCA_variance.png", bbox_inches='tight', dpi=200)
#files.download("PCA_variance.png")

# Save text file
#np.savetxt('fullTimeSeries.txt',binAssignments.T,fmt='%d') 
#files.download("fullTimeSeries.txt")

"""# Shannon Entropy"""

# Generate probability differences
def generatePDFs(X, numberOfBins, binRange=None):
    if not binRange:
       binRange=(X.min(), X.max())
    bins = np.linspace(*binRange, numberOfBins)
    binAssignments = np.digitize(X, bins)
    prob = np.ones(shape=(X.shape[0], numberOfBins))
    for inx, popBinAssignment in enumerate(binAssignments):
        for binNumber in range(1, numberOfBins):
           prob[inx, binNumber] = np.sum(popBinAssignment == binNumber)/X.shape[1] 
    return prob

def entropy(pdf,axis=1):
    with np.errstate(all='ignore'):
        return -1.0*np.sum(np.nan_to_num(pdf*np.log2(pdf)),axis=axis)

# Select counties that want to observe
county_code_1 = '0500000US06037'
county_code_2 = '0500000US08031'
county_code_3 = '0500000US55079'
county_code_4 = '0500000US35001'

name1 = ""
name2 = ""
name3 = ""
name4 = ""
home_vals1 = []
home_vals2 = []
home_vals3 = []
home_vals4 = []
avg_income1 = []
avg_income2 = []
avg_income3 = []
avg_income4 = []
education1 = []
education2 = []
education3 = []
education4 = []
gent1 = []
gent2 = []
gent3 = []
gent4 = []
elig1 = []
elig2 = []
elig3 = []
elig4 = []
X = []
for year in years:
  county1 = merged_df[year].loc[county_code_1]
  county2 = merged_df[year].loc[county_code_2]
  county3 = merged_df[year].loc[county_code_3]
  county4 = merged_df[year].loc[county_code_4]
  name1 = county1.County
  name2 = county2.County
  name3 = county3.County
  name4 = county4.County
  home_vals1.append(county1['Median House Value (Dollars)'])
  home_vals2.append(county2['Median House Value (Dollars)'])
  home_vals3.append(county3['Median House Value (Dollars)'])
  home_vals4.append(county4['Median House Value (Dollars)'])
  avg_income1.append(county1['Median Income (Dollars)'])
  avg_income2.append(county2['Median Income (Dollars)'])
  avg_income3.append(county3['Median Income (Dollars)'])
  avg_income4.append(county4['Median Income (Dollars)'])
  education1.append(county1['Bachelor\'s degree or higher (%)'])
  education2.append(county2['Bachelor\'s degree or higher (%)'])
  education3.append(county3['Bachelor\'s degree or higher (%)'])
  education4.append(county4['Bachelor\'s degree or higher (%)'])
  gent1.append(county1['Gentrified'])
  gent2.append(county2['Gentrified'])
  gent3.append(county3['Gentrified'])
  gent4.append(county4['Gentrified'])
  elig1.append(county1['Eligible'])
  elig2.append(county2['Eligible'])
  elig3.append(county3['Eligible'])
  elig4.append(county4['Eligible'])

# County names
print("County Names:")
print(name1)
print(name2)
print(name3)
print(name4)

# Plot median home value
plt.figure(figsize=(9, 6))
plt.plot(years, home_vals1, label = name1)
plt.plot(years, home_vals2, label = name2)
plt.plot(years, home_vals3, label = name3)
plt.plot(years, home_vals4, label = name4)
plt.legend()
plt.xlabel('Year', fontweight ='bold', fontsize = 12)
plt.ylabel('Median Home Value (USD)', fontweight ='bold', fontsize = 12)
plt.title("County vs County Median Home Value trends")
plt.ticklabel_format(style='plain')
plt.ylim(0,900000)

plt.show()

# Plot average income value
plt.figure(figsize=(9, 6))
plt.plot(years, avg_income1, label = name1)
plt.plot(years, avg_income2, label = name2)
plt.plot(years, avg_income3, label = name3)
plt.plot(years, avg_income4, label = name4)
plt.legend()
plt.xlabel('Year', fontweight ='bold', fontsize = 12)
plt.ylabel('Median Income (USD)', fontweight ='bold', fontsize = 12)
plt.title("County vs County Median Household Income trends")
plt.ticklabel_format(style='plain')
plt.ylim(0,100000)
plt.show()

# Plot education data
plt.figure(figsize=(9, 6))
plt.plot(years, education1, label = name1)
plt.plot(years, education2, label = name2)
plt.plot(years, education3, label = name3)
plt.plot(years, education4, label = name4)
plt.legend()
plt.xlabel('Year', fontweight ='bold', fontsize = 12)
plt.ylabel('Bachelor\'s degree or higher (%)', fontweight ='bold', fontsize = 12)
plt.title("County vs County Education trends")
plt.ticklabel_format(style='plain')
plt.ylim(0,55)
plt.show()

# Check if county is eligible/gentrified
print("Eligible?")
print(elig1)
print(elig2)
print(elig3)
print(elig4)

print("Gentrified?")
print(gent1)
print(gent2)
print(gent3)
print(gent4)

#Single counties shannon entropies - House Values
X = np.array([home_vals1, home_vals2, home_vals3, home_vals4])
numberOfBins = 10
print(X) 
carryingCapacity = 650000

#Single counties shannon entropies - Average Household Incomes
X = np.array([avg_income1, avg_income2, avg_income3, avg_income4])
numberOfBins = 10
print(X) 
carryingCapacity = 50000

#Single counties shannon entropies - Education stats
X = np.array([education1, education2, education3, education4])
numberOfBins = 10
print(X) 
carryingCapacity = 25

N = 3
binRange = (150000, carryingCapacity)
bins = np.linspace(*binRange, numberOfBins)
binAssignments = np.digitize(X, bins)
#print(binRange)
#print(bins)
#print(binAssignments)

fullTimeSeries_pdfs = generatePDFs(X, numberOfBins, binRange)
#print(fullTimeSeries_pdfs)
entFull = entropy(fullTimeSeries_pdfs)
print("Shannon entropyies:")
print(entropy(fullTimeSeries_pdfs))
#print(X[:,:N])
firstNtimesteps_pdfs = generatePDFs(X[:,:N], numberOfBins, binRange)
lastNtimesteps_pdfs  = generatePDFs(X[:,-N:], numberOfBins, binRange)

#np.savetxt('fullTimeSeries.txt', binAssignments.T,fmt='%d')    
#np.savetxt('entropies.txt',entFull,fmt='%d')  
#files.download("entropies.txt")

# Plot shannon Entropies
entsHouse = [2.19715972, 2.28103611, 0.76420451, 0.31199895] 
entsIncome = [1.35164412, 1.30552884, 0.51996667, 0.352211389]
entsEducation = [0.389975, 0.499925, 0.289975, 0.28199895]

plt.figure(figsize=(9, 6))
plt.scatter(["Los Angeles County","Denver County","Milwaukee County","Bernalillo County"], entsHouse, label="Median Home Value")
plt.scatter(["Los Angeles County","Denver County","Milwaukee County","Bernalillo County"], entsIncome, label="Average Income")
plt.scatter(["Los Angeles County","Denver County","Milwaukee County","Bernalillo County"], entsEducation, label="Education")
plt.legend()
plt.xlabel('Counties', fontweight ='bold', fontsize = 12)
plt.ylabel('Shannon Entropy', fontweight ='bold', fontsize = 12)
plt.title("County vs County Shannon Entropies")
plt.ylim(0,2.5)
plt.show()

merged_df[2012].head()

def get_row_values(row_now):
  currval = []
  for year in years:
    for i in range(len(merged_df[year])):
      if i == row_now:
        currval.append(merged_df[year].iloc[i, 4])
  return currval

allvals = []
for i in range((len(merged_df[2011])-4)):
  allvals.append(get_row_values(i))
#print(allvals)

# Calculate all entropies
np_allvals = np.array(allvals, dtype=object)
#print(np_allvals)

binRange = (18, carryingCapacity)
bins = np.linspace(*binRange, numberOfBins)
binAssignments = np.digitize(np_allvals, bins)

all_shannon = []
curr_pdfs = generatePDFs(np_allvals, numberOfBins, binRange)
all_shannon.append(entropy(curr_pdfs))
print(all_shannon[0])

xaxis_ents = range(0, len(all_shannon[0]))
plt.figure(figsize=(9, 6))
plt.scatter(xaxis_ents, all_shannon[0])
plt.legend()
plt.xlabel('Counties', fontweight ='bold', fontsize = 12)
plt.ylabel('Shannon Entropy', fontweight ='bold', fontsize = 12)
plt.title("All Counties Home Value Shannon Entropies")
#plt.title("All Counties Median Income Shannon Entropies")
#plt.title("All Counties Education Shannon Entropies")
plt.ylim(0,3.0)
plt.show()

import seaborn as sns

plt.figure(figsize=(9, 6))
#sns.jointplot(x=xaxis_ents, y=all_shannon[0], kind='hex')
h = sns.jointplot(data=all_shannon[0], kind='hex', height=9)
h.set_axis_labels('Counties', 'Shannon Entropy', fontsize=13, fontweight='bold')
h.figure.tight_layout() 
plt.show()

eligible_only = {}
for year in years:
  df = merged_df[year]
  df = df[df.Gentrified == 0]
  eligible_only[year] = df

gentrified_only = {}
for year in years:
  df = merged_df[year]
  df = df[df.Gentrified == 1]
  gentrified_only[year] = df

neither_only = {}
for year in years:
  df = merged_df[year]
  df = df[df.Gentrified == 0]
  df = df[df.Eligible == 0]
  neither_only[year] = df

# get all gentrified only data set
allvals = []
for i in range((len(neither_only[2011])-4)):
  allvals.append(get_row_values(i))
#print(allvals)

# Calculate all entropies
np_allvals = np.array(allvals, dtype=object)
#print(np_allvals)

binRange = (18, carryingCapacity)
bins = np.linspace(*binRange, numberOfBins)
binAssignments = np.digitize(np_allvals, bins)

all_shannon = []
curr_pdfs = generatePDFs(np_allvals, numberOfBins, binRange)
all_shannon.append(entropy(curr_pdfs))
print(all_shannon[0])

accum = 0
for i in range(len(all_shannon[0])):
  accum = accum + all_shannon[0][i]

average = accum / len(all_shannon[0])
print(average)

"""# MI Venn Diagram"""

from matplotlib.patches import Circle

# Venn diagram of MI         
Xentropies = [0.389975, 0.289975]
Yentropies = [0.499925, 0.28199895]
mutualInfo = [0.2113, 0.0413]
titles=['Denver vs LA','Bernalillo vs Milwaukee ']
fig, axarr = plt.subplots(nrows=1, ncols=2, figsize=(20,6)) 

for ax,hx,hy,mi,ti in zip(np.ravel(axarr),Xentropies,Yentropies,mutualInfo,titles):
    venX = Circle((hx/2-mi/2,0),hx/2, 
                  alpha =0.2, color ='red', label='H(X)')
    venY = Circle((-hy/2+mi/2,0),hy/2, 
                  alpha =0.2, color ='blue', label='H(Y)')
    mutI = Circle((0,0),1,alpha =0.2, color='purple',label='I(X,Y)')
    ax.add_artist(venX);    ax.add_artist(venY); 
    ax.set_title(ti.format(N))

    plt.legend(handles = [venX,venY,mutI])
    ax.set_xlim(-hy+mi/2,hx-mi/2)
    ax.set_ylim(max(hy,hx)/2,max(hy,hx)/-2)
    ax.axis('off')
    plt.savefig("PCA_variance.png", bbox_inches='tight', dpi=300)
    files.download("PCA_variance.png")

"""# PCA"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler


# for year in years: 
cols = ['County', 'Eligible', 'Gentrified', 'Total Population']
df = merged_df[2012].drop(columns=cols)
x = StandardScaler().fit_transform(df)


pca = PCA().fit(x)
principalComponents = pca.fit_transform(x)
# principalDf = pd.DataFrame(data = principalComponents
            # , columns = ['principal component 1', 'principal component 2'])

plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance');

loadings = pd.DataFrame(
    data=pca.components_.T * np.sqrt(pca.explained_variance_), 
    columns=[f'PC{i}' for i in range(1, len(df.columns) + 1)],
    index=df.columns
)
loadings.head()

pc1_loadings = loadings.sort_values(by='PC1', ascending=False)[['PC1']]
pc1_loadings = pc1_loadings.reset_index()
pc1_loadings.columns = ['Attribute', 'CorrelationWithPC1']

plt.bar(x=pc1_loadings['Attribute'], height=pc1_loadings['CorrelationWithPC1'], color='#087E8B')
plt.title('PCA loading scores (first principal component)', size=14)
plt.xticks(rotation='vertical')
plt.ylabel("Loading Scores")
plt.show()

"""# Time-Series Clustering

Attempting to do Multi-Dimensional Dynamic Time warping


"""

df = pd.concat([merged_df[year] for year in years],
                   keys=years).rename_axis(['Year', 'ID'])
df = df.reorder_levels([1,0]).sort_index()
df.head()

mean_income = df.groupby('ID')['Median Income (Dollars)'].apply(np.mean)
mean_house = df.groupby('ID')['Median House Value (Dollars)'].apply(np.mean)

mean_income.var()

print('Mean Income Variance {:30,}'.format(mean_income.var()))
print('Mean Housing Price Variance{:30,}'.format(mean_house.var()))
print(mean_income.describe())
print(mean_house)

# Split the data using stratified sampling
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

# Categorize median housing income into 4 quantile sections
mean_house_df = mean_house.to_frame()
mean_house_df["house_cat"] = pd.cut(mean_house_df['Median House Value (Dollars)'],
                               bins=[0.,
                                     mean_income.quantile(0.25), 
                                     mean_income.quantile(0.5),
                                     mean_income.quantile(0.75), np.inf],
                               labels=[1, 2, 3, 4])

keys = list(mean_house_df.groupby('ID').indices.keys())
strat_train_set_idx = []

# Stratified sample into to splits
for train_index, test_index in split.split(mean_house_df, mean_house_df['house_cat']):
  train_index = train_index.tolist()
  test_index = test_index.tolist()
  strat_train_set_idx = [keys[i] for i in train_index]
  strat_test_set_idx = [keys[i] for i in test_index]

# Split the data
idx = pd.IndexSlice
strat_train_set = df.loc[idx[strat_train_set_idx,:],:]
strat_test_set = df.loc[idx[strat_test_set_idx,:],:]

# Standardize the data
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()

Y = strat_train_set['Gentrified'].to_numpy() # Get target values 
Y_test = strat_test_set['Gentrified'].to_numpy() # Get target values 
X = strat_train_set.drop(['County',
                          'Increased Income',
                          'home_val_percentile',
                          'Eligible',
                          'Gentrified',
                          'Bachelor\'s degree or higher (%)',
                          'Median House Value (Dollars)',
                          'Median Income (Dollars)',
                          'Total households',
                          'Total Population'], axis=1)

X_test = strat_test_set.drop(['County',
                          'Increased Income',
                          'home_val_percentile',
                          'Eligible',
                          'Gentrified',
                          'Bachelor\'s degree or higher (%)',
                          'Median House Value (Dollars)',
                          'Median Income (Dollars)',
                          'Total households',
                          'Total Population'], axis=1)

scaled_X_train = {}
scaled_X_test = {}

for id in strat_test_set_idx:
  scaled_X_test[id] = ss.fit_transform(X_test.loc[id])

for id in strat_train_set_idx:
  scaled_X_train[id] = ss.fit_transform(X.loc[id])

scaled_X_train['0500000US01003']

# Dynamic Time Warping

from fastdtw import dtw

keys = list(scaled_X_train.keys())

D = np.zeros((len(keys), len(keys))) # Stores the distance between each county

# Loop through every potential pairing of counties
for i, id_A in enumerate(strat_train_set_idx):
  for j in range(i+1,len(keys)):
    id_B = keys[j]
                        
    series_A = scaled_X_train[id_A]
    series_B = scaled_X_train[id_B]
    distance, path = dtw(series_A, series_B)
    D[i,j] = distance

heatmap = plt.figure(figsize=(18, 18))
D_graph = D.copy()
D_graph[D_graph == 0] = np.nan
heatmap = sbs.heatmap(D_graph, cmap=sbs.color_palette("viridis", as_cmap=True))
sbs.set(font_scale=1.4)
plt.axis('off')
plt.title('Pairwise county distances')
plt.savefig('dtw.png', dpi=900, bbox_inches='tight')

"""# Polynomial Regression
https://medium.com/magnimetrics/forecasts-with-the-polynomial-regression-model-in-excel-e9120794d342
"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Perceptron

degrees = list(range(1, 5))
scores_arr = np.zeros([len(degrees), 2])


for i, d in enumerate(degrees):
  scores_tmp = np.zeros([100])
  for j in range(10):
    poly = PolynomialFeatures(degree=d)
    X_train_tran = poly.fit_transform(np.concatenate(list(scaled_X_train.values())))
    X_test_tran = poly.fit_transform(np.concatenate(list(scaled_X_test.values())))  
    clf = Perceptron(fit_intercept=False, max_iter=10, tol=None, shuffle=False).fit(X_train_tran, Y)
    scores_tmp[j] = clf.score(X_test_tran, Y_test)

  scores_arr[i, 1] = np.average(scores_tmp)
  scores_arr[i, 0] = d

scores = pd.DataFrame(scores_arr, columns = ['Degree','Average Test Score'])

scores